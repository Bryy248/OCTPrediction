{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec5f2f2",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1725e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd4990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.stats import kurtosis, skew\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from mahotas.features import haralick\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, HalvingGridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a283a",
   "metadata": {},
   "source": [
    "## Dataset and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13107b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'RetinalOCT_Dataset/RetinalOCT_Dataset'\n",
    "classes = ['AMD','CNV','CSR','DME','DR','DRUSEN','MH','NORMAL']\n",
    "\n",
    "HOG_ORIENTATIONS = 9\n",
    "HOG_PIXELS_PER_CELL = (32, 32)\n",
    "HOG_CELLS_PER_BLOCK = (2, 2)\n",
    "\n",
    "LBP_RADIUS = 1\n",
    "LBP_POINTS = 8 * LBP_RADIUS\n",
    "\n",
    "GABOR_THETAS = np.arange(0, np.pi, np.pi/4)\n",
    "\n",
    "USE_PCA = True\n",
    "PCA_VARIANCE = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e79f0",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f706d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(split):\n",
    "    images = []\n",
    "    labels = []\n",
    "    split_path = os.path.join(DATA_PATH, split)\n",
    "    \n",
    "    for i, folder in enumerate(os.listdir(split_path)):\n",
    "        folder_path = os.path.join(split_path, folder)\n",
    "        files = sorted(os.listdir(folder_path))\n",
    "        for file in files:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            img = cv2.imread(file_path)\n",
    "            img = cv2.resize(img, (224, 224))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  \n",
    "            images.append(img)\n",
    "            labels.append(i)\n",
    "    \n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a66212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_oct(image):\n",
    "    # 1. Denoising: bilateral filter lebih halus untuk struktur retina\n",
    "    # denoised = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "\n",
    "    # 1. Denoising: Fast Non-Local Mean Denoising, lebih bagus untuk OCT-Scan karena memiliki struktur yang repetitif\n",
    "    denoised = cv2.fastNlMeansDenoising(image, None, h=10, templateWindowSize=7, searchWindowSize=21)\n",
    "    \n",
    "    # 2. Kontras lokal pakai CLAHE\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    enhanced = clahe.apply(denoised)\n",
    "    \n",
    "    # 3. Normalisasi z-score (lebih stabil dari min-max)\n",
    "    norm = (enhanced - np.mean(enhanced)) / (np.std(enhanced) + 1e-8)\n",
    "    \n",
    "    return enhanced, norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91960ed",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3686ea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for each feature extraction method\n",
    "def extract_intensity_features(image):\n",
    "    return [\n",
    "        np.mean(image),\n",
    "        np.std(image),\n",
    "        skew(image.ravel()),\n",
    "        kurtosis(image.ravel()),\n",
    "        np.percentile(image, 10),\n",
    "        np.percentile(image, 50),\n",
    "        np.percentile(image, 90)\n",
    "    ]\n",
    "\n",
    "def extract_hog_features(image):\n",
    "    return hog(image,\n",
    "               orientations=HOG_ORIENTATIONS,\n",
    "               pixels_per_cell=HOG_PIXELS_PER_CELL,\n",
    "               cells_per_block=HOG_CELLS_PER_BLOCK,\n",
    "               block_norm='L2-Hys',\n",
    "               visualize=False,\n",
    "               feature_vector=True).tolist()\n",
    "\n",
    "def extract_lbp_features(image):\n",
    "    lbp = local_binary_pattern(image, LBP_POINTS, LBP_RADIUS, method='uniform')\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=LBP_POINTS+2, range=(0, LBP_POINTS+2))\n",
    "    hist = hist.astype(float) / (hist.sum() + 1e-8)\n",
    "    return hist.tolist()\n",
    "\n",
    "def extract_haralick_features(image):\n",
    "    haralick_features = haralick(image).mean(axis=0)\n",
    "    return haralick_features.tolist()\n",
    "\n",
    "def extract_gabor_features(image):\n",
    "    features = []\n",
    "    for theta in GABOR_THETAS:\n",
    "        kernel = cv2.getGaborKernel((15, 15), 4.0, theta, 10.0, 0.5, 0)\n",
    "        fimg = cv2.filter2D(image, cv2.CV_32F, kernel)\n",
    "        features.extend([np.mean(fimg), np.std(fimg)])\n",
    "    return features\n",
    "\n",
    "def extract_fourier_features(image):\n",
    "    fft = np.fft.fft2(image)\n",
    "    fft_shift = np.fft.fftshift(fft)\n",
    "    magnitude = np.abs(fft_shift)\n",
    "    return [np.mean(magnitude), np.std(magnitude), np.percentile(magnitude, 75), np.percentile(magnitude, 90)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdbc9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img):\n",
    "    enhanced, norm = preprocess_oct(img)\n",
    "    enhanced_uint8 = enhanced.astype(np.uint8)\n",
    "    features = []\n",
    "\n",
    "    features.extend(extract_hog_features(enhanced_uint8))\n",
    "    features.extend(extract_lbp_features(enhanced_uint8))\n",
    "    features.extend(extract_haralick_features(enhanced_uint8))\n",
    "    features.extend(extract_intensity_features(norm))\n",
    "    features.extend(extract_gabor_features(enhanced_uint8))\n",
    "    features.extend(extract_fourier_features(enhanced_uint8))\n",
    "    \n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a49c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_batch(images, split_name=\"\"):\n",
    "    features = []\n",
    "    total = len(images)\n",
    "\n",
    "    print(f\"\\nExtracting features from {split_name} set\")\n",
    "    for i, img in enumerate(images, 1):\n",
    "        feature = extract_features(img)\n",
    "        features.append(feature)\n",
    "\n",
    "        if i % (total//10) == 0 or i == total:\n",
    "            percentage = int((i/total) * 100)\n",
    "            print(f\"==> Progress: {percentage}% done ({i}/{total} images)\\n\")\n",
    "    \n",
    "    print(f\"Features extraction completed for {split_name} set!\\n\")\n",
    "    return np.vstack(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a693bd52",
   "metadata": {},
   "source": [
    "## Load, Preprocess, Extract Features, Scaling and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee9be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_raw, Y_train = load_image('train')\n",
    "val_img_raw, Y_val = load_image('val')\n",
    "test_img_raw, Y_test = load_image('test')\n",
    "print(f\"Train: {len(train_img_raw)} Images\")\n",
    "print(f\"Validation: {len(val_img_raw)} Images\")\n",
    "print(f\"Test: {len(test_img_raw)} Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dbdc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = extract_features_batch(train_img_raw, \"Train\")\n",
    "X_val = extract_features_batch(val_img_raw, \"Validation\")\n",
    "X_test = extract_features_batch(test_img_raw, \"Test\")\n",
    "print(f\"Raw feature count: {X_train.shape[1]} features per image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ddcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cd60a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA or not\n",
    "if USE_PCA:\n",
    "    pca = PCA(n_components=PCA_VARIANCE, random_state=42)\n",
    "    X_train_scaled = pca.fit_transform(X_train_scaled)\n",
    "    X_val_scaled = pca.transform(X_val_scaled)\n",
    "    X_test_scaled = pca.transform(X_test_scaled)\n",
    "    print(f\"Reduce features from {X_train.shape[1]} to {X_train_scaled.shape[1]} features\")\n",
    "    print(f\"Variance explained: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "else:\n",
    "    print(\"Skipping PCA\")\n",
    "    pca = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adb20e0",
   "metadata": {},
   "source": [
    "## Saving Features, Scaler and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d148ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw and preprocessed features, scaler and PCA\n",
    "\n",
    "with open('features_raw.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'X_train': X_train,\n",
    "        'X_val': X_val,\n",
    "        'X_test': X_test,\n",
    "        'Y_train': Y_train,\n",
    "        'Y_val': Y_val,\n",
    "        'Y_test': Y_test\n",
    "    }, f)\n",
    "print(\"Raw features saved!\")\n",
    "\n",
    "with open('features_scaled.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'X_train_scaled': X_train_scaled,\n",
    "        'X_val_scaled': X_val_scaled,\n",
    "        'X_test_scaled': X_test_scaled,\n",
    "        'Y_train': Y_train,\n",
    "        'Y_val': Y_val,\n",
    "        'Y_test': Y_test\n",
    "    }, f)\n",
    "print(\"Scaled features saved!\")\n",
    "\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"Scaler saved!\")\n",
    "\n",
    "if USE_PCA:\n",
    "    with open('pca.pkl', 'wb') as f:\n",
    "        pickle.dump(pca, f)\n",
    "    print(\"PCA saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52f1d9f",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395e13ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out the \"Load, Preprocess, Extract Features, Scaling and PCA\" part \n",
    "# and load this file if you dont want to extract the feature again\n",
    "\n",
    "with open('features_scaled.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Unpack the dictionary\n",
    "X_train_scaled = data['X_train_scaled']\n",
    "X_val_scaled   = data['X_val_scaled']\n",
    "X_test_scaled  = data['X_test_scaled']\n",
    "\n",
    "Y_train = data['Y_train']\n",
    "Y_val   = data['Y_val']\n",
    "Y_test  = data['Y_test']\n",
    "\n",
    "print(\"Loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf405ae",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bed697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "svm = SVC(class_weight='balanced', random_state=42)\n",
    "\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf', 'poly']\n",
    "}\n",
    "\n",
    "grid_svm = GridSearchCV(\n",
    "    estimator=svm,\n",
    "    param_grid=param_grid_svm,\n",
    "    cv=5,               \n",
    "    n_jobs=1,          \n",
    "    verbose=2,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "\n",
    "grid_svm.fit(X_train_scaled, Y_train)\n",
    "print(\"Best Parameter:\", grid_svm.best_params_)\n",
    "print(\"Best CV Score:\", grid_svm.best_estimator_)\n",
    "best_svm = grid_svm.best_estimator_\n",
    "\n",
    "# Saving Model\n",
    "with open('best_svm_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_svm, f)\n",
    "print(\"Best SVM saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e9a856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction and Metrics\n",
    "y_val_pred_svm = best_svm.predict(X_val_scaled)\n",
    "y_test_pred_svm = best_svm.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nValidation Accuracy:\", accuracy_score(Y_val, y_val_pred_svm))\n",
    "print(\"Test Accuracy:\", accuracy_score(Y_test, y_test_pred_svm))\n",
    "\n",
    "print(\"\\nClassification Report (Validation):\")\n",
    "print(classification_report(Y_val, y_val_pred_svm, target_names=classes))\n",
    "\n",
    "print(\"\\nClassification Report (Test):\")\n",
    "print(classification_report(Y_test, y_test_pred_svm, target_names=classes))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Validation):\")\n",
    "print(confusion_matrix(Y_val, y_val_pred_svm))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Test):\")\n",
    "print(confusion_matrix(Y_test, y_test_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6434d148",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c42541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No PCA for RF\n",
    "with open('features_raw.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    X_train_raw_rf = data['X_train']\n",
    "    X_val_raw_rf = data['X_val']\n",
    "    X_test_raw_rf = data['X_test']\n",
    "    \n",
    "# Scaled\n",
    "scaler_test = StandardScaler()\n",
    "X_train_scaled_rf = scaler_test.fit_transform(X_train_raw_rf)\n",
    "X_val_scaled_rf = scaler_test.transform(X_val_raw_rf)\n",
    "X_test_scaled_rf = scaler_test.transform(X_test_raw_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e3750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 30, 50, 70],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "grid_rf = HalvingGridSearchCV( # use HalvingGridSearchCV for faster searching\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid_rf,\n",
    "    cv=5,               \n",
    "    n_jobs=1,          \n",
    "    verbose=2,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "grid_rf.fit(X_train_scaled_rf, Y_train)\n",
    "print(\"Best Parameter:\", grid_rf.best_params_)\n",
    "print(\"Best CV Score:\", grid_rf.best_estimator_)\n",
    "best_rf = grid_rf.best_estimator_\n",
    "\n",
    "# Saving Model\n",
    "with open('best_rf_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_rf, f)\n",
    "print(\"Best RF saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe7c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction and Metrics\n",
    "y_val_pred_rf = best_rf.predict(X_val_scaled_rf)\n",
    "y_test_pred_rf = best_rf.predict(X_test_scaled_rf)\n",
    "\n",
    "print(\"\\nValidation Accuracy:\", accuracy_score(Y_val, y_val_pred_rf))\n",
    "print(\"Test Accuracy:\", accuracy_score(Y_test, y_test_pred_rf))\n",
    "\n",
    "print(\"\\nClassification Report (Validation):\")\n",
    "print(classification_report(Y_val, y_val_pred_rf, target_names=classes))\n",
    "\n",
    "print(\"\\nClassification Report (Test):\")\n",
    "print(classification_report(Y_test, y_test_pred_rf, target_names=classes))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Validation):\")\n",
    "print(confusion_matrix(Y_val, y_val_pred_rf))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Test):\")\n",
    "print(confusion_matrix(Y_test, y_test_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20a1755",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ea30ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model Function for MLP\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Hidden layer 1\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('units1', min_value=64, max_value=512, step=64),\n",
    "        activation='relu',\n",
    "        input_shape=(X_train_scaled.shape[1],)\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(hp.Float('dropout1', 0.2, 0.5, step=0.1)))\n",
    "\n",
    "    # Hidden layer 2\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('units2', min_value=32, max_value=256, step=32),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(hp.Float('dropout2', 0.2, 0.5, step=0.1)))\n",
    "\n",
    "    # Hidden layer 3\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('units3', min_value=16, max_value=128, step=16),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(hp.Float('dropout3', 0.2, 0.5, step=0.1)))\n",
    "\n",
    "    # Output layer (jumlah neuron = jumlah kelas)\n",
    "    model.add(Dense(len(classes), activation='softmax'))\n",
    "\n",
    "    # Gunakan categorical_crossentropy karena label sudah one-hot\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a587f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory='mlp_tuning',\n",
    "    project_name='mlp_randomsearch'\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    X_train_scaled, Y_train,\n",
    "    validation_data=(X_val_scaled, Y_val),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2295f148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Best Hyperparameter from the search\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param, value in best_hp.values.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3682e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the model with the best parameters\n",
    "\n",
    "best_mlp = build_model(best_hp)\n",
    "best_mlp.summary()\n",
    "\n",
    "# Callback for early stopping, and modelcheckpoint to save the model\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_mlp_model.keras', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "# Train back the model with the best parameter\n",
    "history = best_mlp.fit(\n",
    "    X_train_scaled, Y_train,\n",
    "    validation_data=(X_val_scaled, Y_val),\n",
    "    epochs=30, #100\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad41f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for accuracy\n",
    "val_loss, val_acc = best_mlp.evaluate(X_val_scaled, Y_val, verbose=2)\n",
    "test_loss, test_acc = best_mlp.evaluate(X_test_scaled, Y_test, verbose=2)\n",
    "\n",
    "print(\"Validation Accuracy:\", val_acc)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972e4529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction and Metrics\n",
    "y_val_pred_mlp = np.argmax(best_mlp.predict(X_val_scaled), axis=1)\n",
    "y_test_pred_mlp = np.argmax(best_mlp.predict(X_test_scaled), axis=1)\n",
    "\n",
    "print(\"\\nClassification Report (Validation):\")\n",
    "print(classification_report(Y_val, y_val_pred_mlp, target_names=classes))\n",
    "\n",
    "print(\"\\nClassification Report (Test):\")\n",
    "print(classification_report(Y_test, y_test_pred_mlp, target_names=classes))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Validation):\")\n",
    "print(confusion_matrix(Y_val, y_val_pred_mlp))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Test):\")\n",
    "print(confusion_matrix(Y_test, y_test_pred_mlp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer_vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
